{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '..')\n",
    "import importlib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/kidney-exchange/survey2_data.csv')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of pairwise comparisons per voter? Easy - just as many as the number of unique voters.\n",
    "\n",
    "Save the various voter strategies for later, if necessary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies = pd.concat([df[\"Voter Strategy 0\"].dropna(), df[\"Voter Strategy 1\"].dropna()])\n",
    "# strategies.to_csv(\"../data/kidney-exchange/survey2_strategy.csv\", index=False)\n",
    "sc = pd.read_csv(\"../data/kidney-exchange/survey2_strategy.csv\")\n",
    "## 0 - no ranking (strategy not used); 1-3 ranked choice of strategy; negative magnitude means acted opposite the heuristic\n",
    "sc['Older'] = np.abs(sc['Younger'][sc['Younger'] < 0])\n",
    "sc['Younger'].loc[sc['Younger'] < 0,] = 0\n",
    "sc['Drink more'] = np.abs(sc['Drink less'][sc['Drink less'] < 0])\n",
    "sc['Drink less'].loc[sc['Drink less'] < 0,] = 0\n",
    "sc['Unhealthy'] = np.abs(sc['Healthy'][sc['Healthy'] < 0])\n",
    "sc['Healthy'].loc[sc['Healthy'] < 0,] = 0\n",
    "options = ['Older', 'Younger', 'Drink more', 'Drink less', 'Unhealthy', 'Healthy']\n",
    "# number of people with no heuristics\n",
    "((sc[options] == 0) | (sc[options].isna())).all(1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def borda(x, key):\n",
    "    count = 0\n",
    "    key_val = x[key]\n",
    "    if key_val == 0: return 0\n",
    "    for val in x[[o for o in options if o != key]]:\n",
    "        if val == 0 or key_val < val:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "sc = sc.fillna(0)\n",
    "borda = pd.DataFrame([sc.apply(borda, axis=1, args=(key,)) for key in options]).transpose()\n",
    "borda.columns = options\n",
    "borda.to_csv(\"../figures/data/ke-borda.csv\")\n",
    "borda.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0 = df[[\n",
    "    '1x2', '1x3', '1x4', '1x5', '1x6', '1x7', '1x8', '2x3', '2x4', '2x5', '2x6', '2x7', '2x8', '3x4', \n",
    "    '3x5', '3x6', '3x7', '3x8', '4x5', '4x6', '4x7', '4x8', '5x6', '5x7', '5x8', '6x7', '6x8', '7x8'\n",
    "]].dropna()\n",
    "df_1 = df[[\n",
    "    '2x1', '3x1', '4x1', '5x1', '6x1', '7x1', '8x1', '3x2', '4x2', '5x2', '6x2', '7x2', '8x2', '4x3', \n",
    "    '5x3', '6x3', '7x3', '8x3', '5x4', '6x4', '7x4', '8x4', '6x5', '7x5', '8x5', '7x6', '8x6', '8x7', \n",
    "]].dropna()\n",
    "df_0.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_0.shape[0])\n",
    "print(df_1.shape[0])\n",
    "print(df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "1. Hardcode the patient profiles.\n",
    "2. Replace each pairwise comparison with the actual patient features, and translate the label. Preserve exogenous features.\n",
    "\n",
    "### Patient Profiles\n",
    "- ID: patient #\n",
    "- Age: age in years\n",
    "- Alcohol consumption: number of drinks per day\n",
    "- Skin cancer in remission: whether or not the patient has skin cancer in remission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_endo = [\"ID\", \"Abbrv.\", \"Initials\", \"Age\", \"AlcoholConsumption\", \"SkinCancer\"]\n",
    "patients = pd.DataFrame([\n",
    "    [1, \"YRH\", \"W.A.\", 30, 1, 0],\n",
    "    [2, \"YFH\", \"V.S.\", 30, 5, 0],\n",
    "    [3, \"YRC\", \"J.B.\", 30, 1, 1],\n",
    "    [4, \"YFC\", \"K.D.\", 30, 5, 1],\n",
    "    [5, \"ORH\", \"Y.D.\", 70, 1, 0],\n",
    "    [6, \"OFH\", \"J.F.\", 70, 5, 0],\n",
    "    [7, \"ORC\", \"M.K.\", 70, 1, 1],\n",
    "    [8, \"OFC\", \"R.F.\", 70, 5, 1]\n",
    "], columns=features_endo)\n",
    "features_endo.remove(\"Abbrv.\")\n",
    "patients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import re\n",
    "\n",
    "rows = []\n",
    "regexp = re.compile(r'^Patient ([A-Z.]{4})')\n",
    "for df in [df_0, df_1]:\n",
    "    for column in df.columns:\n",
    "        choices_num = [int(x) for x in column.split(\"x\")]\n",
    "        choices = patients[patients[\"ID\"]\\\n",
    "            .isin(choices_num)]\\\n",
    "            .sort_values(\"ID\", ascending=(choices_num[0] < choices_num[1]))\\\n",
    "            .reset_index()\n",
    "        for i, choice in enumerate(df[column].values):\n",
    "            row = {\n",
    "                \"{}_{}\".format(f, s): choices[f].iloc[s]\n",
    "                for f in choices.columns for s in range(2)\n",
    "            }\n",
    "            row['VoterID'] = i\n",
    "            chosen_initials = regexp.search(choice).group(1)\n",
    "            row['Chosen'] = choices.index[choices[\"Initials\"] == chosen_initials][0]\n",
    "            rows.append(row)\n",
    "\n",
    "df_pairwise = pd.DataFrame(rows)\n",
    "df_pairwise = df_pairwise.set_index([df_pairwise.index, 'VoterID'])\n",
    "df_pairwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"{}_{}\".format(f, s) for f in features_endo for s in range(2) if f != \"ID\" and f != \"Initials\"]\n",
    "num_features = features\n",
    "cat_features = []\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pairwise.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proc = df_pairwise.loc[:,features + [\"Chosen\"]]\n",
    "# convert to numeric, changing literals to NaN\n",
    "for f in num_features:\n",
    "    df_proc.loc[:,f] = pd.to_numeric(df_proc.loc[:,f], errors='coerce')\n",
    "df_proc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proc.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hmm.classification\n",
    "importlib.reload(hmm.classification)\n",
    "from hmm.classification import train_test_val_dev_split\n",
    "\n",
    "X = df_proc.drop(labels=[\"Chosen\"], axis='columns', inplace=False)\n",
    "y = df_proc[\"Chosen\"]\n",
    "X_train, X_test, X_val, X_dev, y_train, y_test, y_val, y_dev = train_test_val_dev_split(X, y)\n",
    "display(X_train.head())\n",
    "display(y_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What labeling functions to write?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies.sample().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hmm.labeling.kidney_exchange as ke\n",
    "import hmm.labeling.models\n",
    "import hmm.labeling.utils\n",
    "importlib.reload(hmm.labeling.kidney_exchange)\n",
    "importlib.reload(hmm.labeling.models)\n",
    "importlib.reload(hmm.labeling.utils)\n",
    "\n",
    "from hmm.labeling.models import Labeler\n",
    "from snorkel.labeling import LFAnalysis\n",
    "\n",
    "lfs = [\n",
    "    ke.age,\n",
    "    ke.alcohol,\n",
    "    ke.health\n",
    "]\n",
    "\n",
    "labeler = Labeler(lfs)\n",
    "L_train, L_dev, L_val = labeler.label([X_train, X_dev, X_val])\n",
    "LFAnalysis(L=L_dev, lfs=lfs).lf_summary(Y=y_dev.values).sort_values(\"Correct\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Experiment: LF Analysis*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the validation set (since tuning is done)\n",
    "analysis = LFAnalysis(L=L_val, lfs=lfs).lf_summary(Y=y_val.values)\n",
    "analysis.to_csv(\"../figures/data/ke-lfanalysis.csv\")\n",
    "# labeling density\n",
    "pd.DataFrame(L_dev, columns=[lf.name for lf in lfs]).to_csv(\"../figures/data/ke-density.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import MajorityLabelVoter\n",
    "\n",
    "class WeightedMajorityLabelVoter(MajorityLabelVoter):\n",
    "    def predict_proba(self, L: np.ndarray) -> np.ndarray:\n",
    "        n, m = L.shape\n",
    "        Y_p = np.zeros((n, self.cardinality))\n",
    "        for i in range(n):\n",
    "            counts = np.zeros(self.cardinality)\n",
    "            for j in range(m):\n",
    "                if L[i, j] != -1:\n",
    "                    # add a weighted count instead of a whole count\n",
    "                    counts[L[i, j]] += self.mu[j] + self.beta\n",
    "            Y_p[i, :] = np.where(counts == max(counts), 1, 0)\n",
    "        Y_p /= Y_p.sum(axis=1).reshape(-1, 1)\n",
    "        return Y_p\n",
    "    \n",
    "    def set_mu(self, borda):\n",
    "        mu = borda.mean()[[\"Younger\", \"Drink less\", \"Healthy\"]].values\n",
    "        self.mu = mu\n",
    "        \n",
    "    def set_beta(self, beta):\n",
    "        self.beta = beta\n",
    "        \n",
    "\n",
    "model_majority = MajorityLabelVoter()\n",
    "preds_train = model_majority.predict(L=L_train)\n",
    "\n",
    "model_majority_weighted = WeightedMajorityLabelVoter()\n",
    "model_majority_weighted.set_mu(borda)\n",
    "model_majority_weighted.set_beta(0)\n",
    "preds_train = model_majority_weighted.predict(L=L_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Experiment: LF weighting*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cardinality is num classes\n",
    "importlib.reload(hmm.labeling.models)\n",
    "\n",
    "## normal fitting\n",
    "model_label = labeler.fit(L_train, Y_dev=y_dev.values)\n",
    "## TODO: try setting learned heuristic weights as initial weights for model with mu_init\n",
    "# labeler.model.mu_init = ...\n",
    "# model_label = labeler.fit(L_train, Y_dev=y_dev.values)\n",
    "analysis = LFAnalysis(L=L_val, lfs=lfs).lf_summary(Y=y_val.values)\n",
    "analysis['weight'] = pd.Series(model_label.get_weights(), index=[lf.name for lf in lfs])\n",
    "analysis.to_csv('../figures/data/ke-weights.csv')\n",
    "analysis.sort_values('Emp. Acc.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the label model accuracy per scenario type?\n",
    "# create a dataframe with scenariotype, gold label, probabilistic label, votes for each LF\n",
    "\n",
    "preds_scenario = pd.DataFrame(L_val, columns=[lf.name for lf in lfs])\n",
    "\n",
    "def calc_scenario_type(row):\n",
    "    age = row[\"Age_0\"] != row[\"Age_1\"]\n",
    "    alcohol = row[\"AlcoholConsumption_0\"] != row[\"AlcoholConsumption_1\"]\n",
    "    cancer = row[\"SkinCancer_0\"] != row[\"SkinCancer_1\"]\n",
    "    \n",
    "    if (age and alcohol and cancer): return \"Random\"\n",
    "    if (age and alcohol): return \"Age + Drinking\"\n",
    "    if (age and cancer): return \"Age + Health\"\n",
    "    if (alcohol and cancer): return \"Drinking + Health\"\n",
    "    if (age): return \"Age\"\n",
    "    if (alcohol): return \"Drinking\"\n",
    "    if (cancer): return \"Health\"\n",
    "    \n",
    "preds_scenario['scenario'] = X_val.apply(calc_scenario_type, axis=1).values\n",
    "preds_scenario['actual'] = y_val.values\n",
    "probs = labeler.model.predict_proba(L=L_val)\n",
    "probs_weighted_majority = model_majority.predict_proba(L=L_val)\n",
    "preds_scenario['pred'] = Labeler.probs_to_preds(probs)\n",
    "preds_scenario['pred_weighted_majority'] = Labeler.probs_to_preds(probs_weighted_majority)\n",
    "preds_scenario.to_csv(\"../figures/data/ke-preds_scenario.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(hmm.labeling.models)\n",
    "from hmm.labeling.models import Labeler\n",
    "\n",
    "for model in [model_majority_weighted, model_majority, model_label]:\n",
    "    Labeler.score(model, L_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compare our accuracy to Freedman et al. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_scores = pd.DataFrame({\n",
    "    'j': [1, 3, 2, 5, 4, 7, 6, 8],\n",
    "    'direct': [1, .236280167, 0.103243396, 0.070045054, 0.035722844, 0.024072427, 0.011349772, 0.002769801],\n",
    "    'attribute-based': [1, 0.13183083, 0.29106507, 0.03837135, 0.08900390, 0.01173346, 0.02590593, 0.00341520]\n",
    "})\n",
    "patient_scores = patients.set_index('ID').join(profile_scores.set_index('j'))\n",
    "patient_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def get_profile_score(x, attribute=False):\n",
    "    # return the profile matching this patient's profile\\\n",
    "    x.index = [l.split(\"_\")[0] for l in x.index]\n",
    "    profile = patient_scores[\n",
    "        (patient_scores['Age'] == x['Age']) & (patient_scores['AlcoholConsumption'] == x['AlcoholConsumption']) & (patient_scores['SkinCancer'] == x['SkinCancer'])\n",
    "    ].reset_index()\n",
    "    return profile.loc[0, 'direct' if not attribute else 'attribute-based']\n",
    "\n",
    "def classify_weighted(x):\n",
    "    return np.argmax([get_profile_score(x[[c for c in X_train.columns if c.split(\"_\")[1] == str(i)]]) for i in range(2)])\n",
    "\n",
    "accuracy_score(y_true=y_val.values, y_pred=X_val.apply(classify_weighted, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hmm.classification\n",
    "importlib.reload(hmm.classification)\n",
    "from hmm.classification import Classifier\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "clf = Classifier(features, num_features, cat_features)\n",
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "freedman_pred = X.apply(classify_weighted, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(freedman_pred == model_majority_weighted.predict(labeler.label(X))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kf_cross_val(kf, X_n, y_n):\n",
    "    gold_acc = []\n",
    "    lm_acc = []\n",
    "    freedman_acc = []\n",
    "    borda_acc = []\n",
    "    mv_acc = []\n",
    "    mv_weighted_acc = []\n",
    "    \n",
    "    for i_train, i_test in kf.split(X_n):\n",
    "        # train/test split by fold\n",
    "        X_train_n, X_test_n = X_n.iloc[i_train], X_n.iloc[i_test]\n",
    "        y_train_n, y_test_n = y_n.iloc[i_train], y_n.iloc[i_test]\n",
    "        freedman_pred_n = freedman_pred.iloc[i_test]\n",
    "        \n",
    "        # gold accuracy\n",
    "        clf.fit(X_train_n, y_train_n)\n",
    "        gold_acc.append(clf.score(X_test_n, y_test_n, verbose=False))\n",
    "        \n",
    "        # lm accuracy\n",
    "        ## train label model\n",
    "        labeler = Labeler(lfs)\n",
    "        L_train_n = labeler.label(X_train_n, verbose=False)\n",
    "        labeler.fit(L_train_n, Y_dev=y_train_n)\n",
    "        print(labeler.model.get_weights())\n",
    "        ## label points in X_train\n",
    "        X_train_filtered_n, probs_train_filtered_n = labeler.filter_probs(X_train_n, L_train_n)\n",
    "        preds_train_filtered_n = Labeler.probs_to_preds(probs_train_filtered_n)\n",
    "        ## fit and score\n",
    "        clf.fit(X_train_filtered_n, preds_train_filtered_n)\n",
    "        lm_acc.append(clf.score(X_test_n, y_test_n, verbose=False))\n",
    "        \n",
    "        # weighted majority voter accuracy\n",
    "        model_majority_weighted = WeightedMajorityLabelVoter()\n",
    "        model_majority_weighted.set_mu(borda)\n",
    "        model_majority_weighted.set_beta(0)\n",
    "        preds_train = model_majority_weighted.predict(L=L_train_n)\n",
    "        clf.fit(X_train_n, preds_train)\n",
    "        borda_acc.append(clf.score(X_test_n, y_test_n, verbose=False))\n",
    "        \n",
    "        freedman_acc.append(accuracy_score(y_true=y_test_n.values, y_pred=freedman_pred_n))\n",
    "        \n",
    "        L_test_n = labeler.label(X_test_n, verbose=False)\n",
    "        mv_acc.append(accuracy_score(y_true=y_test_n, y_pred=model_majority.predict(L=L_test_n)))\n",
    "        mv_weighted_acc.append(accuracy_score(y_true=y_test_n, y_pred=model_majority_weighted.predict(L=L_test_n)))\n",
    "        \n",
    "    return gold_acc, lm_acc, freedman_acc, borda_acc, mv_acc, mv_weighted_acc\n",
    "\n",
    "gold_acc, lm_acc, freedman_acc, borda_acc, mv_acc, mv_weighted_acc = kf_cross_val(kf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "\n",
    "#### Training on Gold Labels\n",
    "\n",
    "Using just the labels (no label model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_score_interval(accs):\n",
    "    print(\"{} +/- {}\".format(np.mean(accs), np.std(accs)/len(accs)))\n",
    "\n",
    "print_score_interval(gold_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training on Heuristic Labels\n",
    "\n",
    "Using the label model, filter out unlabeled points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_score_interval(lm_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted majority voting, with classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_score_interval(borda_acc)\n",
    "print(borda_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Just the unweighted voting models, no classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_score_interval(mv_acc)\n",
    "print_score_interval(mv_weighted_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline - Freedman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_score_interval(freedman_acc)\n",
    "print(freedman_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "*Experiment: Accuracy by Sample Size*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "clf = Classifier(features, num_features, cat_features)\n",
    "results = []\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=11)\n",
    "\n",
    "for n in list(range(50, 1000, 10))+list(range(1100,8100,100)):\n",
    "    sample = X.sample(n).index\n",
    "    X_n = X.loc[sample]\n",
    "    y_n = y.loc[sample]\n",
    "    \n",
    "    gold_acc, lm_acc, freedman_acc = kf_cross_val(kf, X_n, y_n)\n",
    "    \n",
    "    # store results\n",
    "    res = (n, np.mean(gold_acc), np.std(gold_acc), np.mean(lm_acc), np.std(lm_acc), np.mean(freedman_acc), np.std(freedman_acc))\n",
    "    print(res)\n",
    "    results.append(res)\n",
    "pd.DataFrame(results, columns=[\"n_rows\", \"acc_gold\", \"std_gold\", \"acc_heuristic\", \"std_heuristic\", \"acc_freedman\", \"std_freedman\"]).to_csv(\"../figures/data/ke-accs_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Experiment: Accuracy by Number of Voters*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "clf = Classifier(features, num_features, cat_features)\n",
    "model_majority = MajorityLabelVoter()\n",
    "model_majority_weighted = WeightedMajorityLabelVoter()\n",
    "data = []\n",
    "users = df_proc.groupby(['VoterID'])\n",
    "a = np.arange(users.ngroups)\n",
    "\n",
    "trials = 200\n",
    "epochs = 50\n",
    "np.random.seed(21)\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    print(\"# Epoch {} #\".format(i))\n",
    "\n",
    "    acc_gold = []\n",
    "    acc_lm = []\n",
    "    acc_borda = []\n",
    "    \n",
    "    # shuffle the voter IDs\n",
    "    np.random.shuffle(a)\n",
    "    \n",
    "    num_range = list(range(1,5))+list(range(5,trials,5))\n",
    "    for n in num_range:\n",
    "#         print(\"## Testing N={}##\".format(n)) \n",
    "        \n",
    "        # choose n random respondents\n",
    "        n_respondents = df_proc[users.ngroup().isin(a[:n])]\n",
    "        # get the borda counts from each respondent\n",
    "        borda_n = borda.loc[n_respondents.index.get_level_values('VoterID').unique(),]\n",
    "\n",
    "        # get the indices of all data from the selected respondents\n",
    "        train_index = n_respondents.groupby(level='VoterID').apply(lambda x: x.sample(20)).reset_index(level=0).index\n",
    "        # train test split - train on the selected respondents, test on the rest\n",
    "        X_train, y_train = X.loc[train_index], y.loc[train_index]\n",
    "        X_test, y_test = X.loc[~X.index.isin(train_index)], y.loc[~y.index.isin(train_index)]\n",
    "        # label the data\n",
    "        L_train, L_test = labeler.label([X_train, X_test], verbose=False)\n",
    "\n",
    "        # baseline gold model accuracy\n",
    "        clf.fit(X_train, y_train)\n",
    "        acc_gold.append(clf.score(X_test, y_test, verbose=False))\n",
    "\n",
    "        # label model accuracy\n",
    "        X_train_filtered, probs_train_filtered = labeler.filter_probs(X_train, L_train)\n",
    "        preds_train_filtered = Labeler.probs_to_preds(probs_train_filtered)\n",
    "        clf.fit(X_train_filtered, preds_train_filtered)\n",
    "        acc_lm.append(clf.score(X_test, y_test, verbose=False))\n",
    "        \n",
    "        # weighted majority voter accuracy\n",
    "        model_majority_weighted.set_mu(borda_n)\n",
    "        model_majority_weighted.set_beta(0)\n",
    "        preds_train = model_majority_weighted.predict(L=L_train)\n",
    "        clf.fit(X_train, preds_train)\n",
    "        acc_borda.append(clf.score(X_test, y_test, verbose=False))\n",
    "\n",
    "    data.append([ \n",
    "        acc_gold, \n",
    "        acc_lm,\n",
    "        acc_borda\n",
    "    ])\n",
    "summary = np.array(data)\n",
    "stats = np.concatenate((np.array([num_range]), np.mean(summary, axis=0), np.std(summary, axis=0)), axis=0).transpose()\n",
    "accs = pd.DataFrame(stats, columns=[\"n_voters\", \"acc_gold\", \"acc_heuristic\", \"acc_borda\", \"std_gold\", \"std_heuristic\", \"std_borda\"])\n",
    "accs['n_voters'] = accs['n_voters'].astype(int)\n",
    "accs.to_csv(\"../figures/data/ke-accs_voters_icml.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs[['acc_gold', 'acc_heuristic', 'acc_borda']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs[['std_gold', 'std_heuristic', 'std_borda']].plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

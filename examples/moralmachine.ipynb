{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '..')\n",
    "import importlib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from snorkel.labeling import labeling_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access SQL DB with data\n",
    "engine = create_engine(\"sqlite:///../data/moralmachine.db\", echo=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query random sample of responses grouped by ResponseID; only take responses for which both instances are present\n",
    "sample_size = 100000000\n",
    "query = \"\"\"\n",
    "    SELECT gc.session_count, sr.* FROM sharedresponses sr\n",
    "    INNER JOIN (\n",
    "        SELECT ExtendedSessionID, COUNT(DISTINCT ResponseID) AS session_count FROM sharedresponses\n",
    "        WHERE UserID <> ''\n",
    "        \n",
    "        /* Get only full sessions. */\n",
    "        GROUP BY ExtendedSessionID\n",
    "            HAVING COUNT(DISTINCT ResponseID) LIKE 13\n",
    "                AND COUNT(ResponseID) LIKE 26\n",
    "        \n",
    "        LIMIT {0:d}\n",
    "    ) gc\n",
    "        ON gc.ExtendedSessionID = sr.ExtendedSessionID\n",
    "    ORDER BY sr.UserID\n",
    "\"\"\".format(sample_size)\n",
    "df = pd.read_sql(query, con=engine)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size and other stuff\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of users\n",
    "df['UserID'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison to Noothigattu et al., how many pairwise comparisons per voter in this sample dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of pairwise comparisons per voter?\n",
    "# = number of response IDs per voter\n",
    "df.groupby('UserID')['ResponseID'].nunique().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of scenarios\n",
    "pd.DataFrame(df['ScenarioType'].value_counts()/df['ScenarioType'].value_counts().sum()).to_csv(\"../figures/data/freq_scenario.csv\")\n",
    "# frequency of each character count\n",
    "characters = ['Man', 'Woman', 'Pregnant', 'Stroller', 'OldMan', 'OldWoman', 'Boy', 'Girl', 'Homeless', 'LargeWoman', 'LargeMan', 'Criminal', 'MaleExecutive', 'FemaleExecutive', 'FemaleAthlete', 'MaleAthlete', 'FemaleDoctor', 'MaleDoctor', 'Dog', 'Cat']\n",
    "pd.DataFrame(df[characters].sum()/df[characters].sum().sum(), columns=['frequency']).to_csv('../figures/data/freq_character.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user countries\n",
    "freqs = df['UserCountry3'].value_counts()\n",
    "pd.DataFrame(freqs/freqs.sum()).to_csv('../figures/data/freq_countries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:26,:].sort_values('ScenarioOrder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the variables above, these are the variables that vary within response pairs:\n",
    "> 'NumberOfCharacters', 'DiffNumberOfCharacters', 'Saved', 'Template', 'DescriptionShown',\n",
    "'LeftHand', 'UserCountry3', 'Man', 'Woman', 'Pregnant', 'Stroller',\n",
    "'OldMan', 'OldWoman', 'Boy', 'Girl', 'Homeless', 'LargeWoman',\n",
    "'LargeMan', 'Criminal', 'MaleExecutive', 'FemaleExecutive',\n",
    "'FemaleAthlete', 'MaleAthlete', 'FemaleDoctor', 'MaleDoctor', 'Dog',\n",
    "'Cat'\n",
    "\n",
    "N.B. in each pair of responses, only one is the result of an intervention, and only one is saved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"ResponseID\",\"Intervention\",\"Saved\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "First, let's convert to abstract features for ease of labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hmm.labeling.utils\n",
    "importlib.reload(hmm.labeling.utils)\n",
    "from hmm.labeling.utils import transform_abstract\n",
    "\n",
    "ids = ['ResponseID', 'ExtendedSessionID', 'UserID']\n",
    "df_i = df.set_index(ids, append=True, verify_integrity=True)\n",
    "df_i.index = df_i.index.set_names(['UUID']+ids)\n",
    "df_abstract = transform_abstract(df_i)\n",
    "df_abstract.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check our work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df_i.sample().index\n",
    "df_i.loc[sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abstract.loc[sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abstract.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, select the fields that are unique to each scenario (the fields that vary within pairs of responses). Then split the dataset into two disjoint sets of alternatives: one in which an intervention occurs, and one in which there is no intervention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [\"ResponseID\", \"ExtendedSessionID\", \"UserID\"]\n",
    "scenario_fields = [\n",
    "    'Saved', 'NumberOfCharacters', 'DescriptionShown', 'LeftHand', 'Male', 'Female', 'Young', 'Old', 'Infancy', 'Pregnancy',\n",
    "    'Fat', 'Fit', 'Working', 'Medical', 'Homeless', 'Criminal', 'Human',\n",
    "    'Non-human', 'Passenger', 'Law Abiding', 'Law Violating'\n",
    "]\n",
    "\n",
    "df_abstract = df_abstract.reset_index(level='UUID')\n",
    "sample_response = df_abstract.sample().index\n",
    "intervention = df_abstract[df_abstract['Intervene'] == 1][scenario_fields]\n",
    "no_intervention = df_abstract[df_abstract['Intervene'] == 0][scenario_fields]\n",
    "\n",
    "print(\"Alternative w/ intervention:\")\n",
    "display(intervention.loc[sample_response])\n",
    "print(\"Alternative w/o intervention:\")\n",
    "display(no_intervention.loc[sample_response])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, combine the datasets on response ID, separating the variable characteristics with suffixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_endo = intervention.join(no_intervention, lsuffix='_int', rsuffix='_noint', how='inner')\n",
    "df_endo.loc[sample_response]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data for the columns from the raw data that _didn't_ change within response pairs. Remember to eliminate duplicate response pairs - now that we have a combined tuple for each pairwise comparison, they're no longer necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exo = df_abstract[[col for col in df_abstract.columns if col not in scenario_fields]]\n",
    "df_exo = df_exo.loc[~df_exo.index.duplicated(keep='first')]\n",
    "df_exo.loc[sample_response]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then join that data in with the combined endogenous variables to get a full tuple for each pairwise comparison presented to a user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined = df_exo.join(df_endo, how='inner').set_index(['UUID'], append=True, verify_integrity=True)\n",
    "sample = df_joined.sample().index\n",
    "df_joined.loc[sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hmm.labeling.utils\n",
    "importlib.reload(hmm.labeling.utils)\n",
    "from hmm.labeling.utils import pictofy\n",
    "    \n",
    "# works with abstract\n",
    "pictofy(df_joined.loc[sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much data did we lose with all these joins? Shouldn't be any - all we're doing is dividing the dataset in half."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"df: {}\".format(df.shape))\n",
    "print(\"df_endo: {}\".format(df_endo.shape))\n",
    "print(\"df_exo: {}\".format(df_exo.shape))\n",
    "print(\"df_joined: {}\".format(df_joined.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's make it easier to interpret the target variable. For each response, we know whether the user chose to save one set of characters (\\_int) by intervention, or save another set (\\_noint) by not intervening. Let's call that variable \"Intervened\" to indicate whether or not the user intervened (swerved the AV)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined['Intervened'] = (df_joined['Saved_int'] == 1).astype(int)\n",
    "df_joined = df_joined.drop(axis='columns', labels=['Saved_{}'.format(s) for s in ['int', 'noint']]+[\"Intervene\"])\n",
    "scenario_fields.remove('Saved')\n",
    "df_joined.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now choose which features to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include user countries and other metadata or not? decided not - experts writing LFs are trying to decide most moral response, not predict what an individual from a certain background would do\n",
    "# but if it helps generalization, technically useful... try both?\n",
    "target = [\"Intervened\"]\n",
    "features = [\"Template\", \"UserCountry3\"] + [\"{}_{}\".format(f, s) for f in scenario_fields for s in [\"int\", \"noint\"]]\n",
    "cat_features = [\n",
    "    \"Template\", \"UserCountry3\"\n",
    "]\n",
    "num_features = [f for f in features if f not in cat_features]\n",
    "df_joined[features].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now explicitly type cast and deal with NA's or missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform numerical data types\n",
    "df_proc = df_joined.loc[:, features + target]\n",
    "\n",
    "def transform_num(df, num_features):\n",
    "    # convert to numeric, changing literals to NaN\n",
    "    for f in num_features:\n",
    "        df.loc[:, f] = pd.to_numeric(df.loc[:, f], errors='coerce')\n",
    "    # are there any nan in the numerical features? usually just one\n",
    "    nan = df[df[num_features].isna().any(axis=1)]\n",
    "    print(\"Dropping these NaN:\")\n",
    "    display(nan)\n",
    "    return df.dropna(axis=0, how='any', subset=num_features)\n",
    "\n",
    "df_proc = transform_num(df_proc, num_features)\n",
    "df_proc.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A standard train test split for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hmm.classification\n",
    "importlib.reload(hmm.classification)\n",
    "from hmm.classification import train_test_val_dev_split\n",
    "\n",
    "def make_X_y(df):\n",
    "    X = df.drop(labels=[\"Intervened\"], axis='columns', inplace=False)\n",
    "    y = df[\"Intervened\"]\n",
    "    return X, y\n",
    "\n",
    "X, y = make_X_y(df_proc)\n",
    "X_train, X_test, X_val, X_dev, y_train, y_test, y_val, y_dev = train_test_val_dev_split(X, y)\n",
    "display(X_train.head())\n",
    "display(y_train.head())\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_val.shape)\n",
    "print(X_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many unique scenarios are there (vs total scenarios)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "counts = np.unique(X.values, axis=0, return_counts=True)\n",
    "stats.describe(counts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some sample labeling functions, constructed with the help of the effect sizes in the Moral Machine experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import hmm.labeling.moralmachine as mm\n",
    "import hmm.labeling.models\n",
    "import hmm.labeling.utils\n",
    "importlib.reload(hmm.labeling.moralmachine)\n",
    "importlib.reload(hmm.labeling.models)\n",
    "importlib.reload(hmm.labeling.utils)\n",
    "import hmm.labeling.moralmachine as mm\n",
    "import hmm.labeling.models\n",
    "import hmm.labeling.utils\n",
    "\n",
    "from hmm.labeling.models import Labeler\n",
    "from snorkel.labeling import LFAnalysis\n",
    "\n",
    "lfs = [\n",
    "    mm.doctors,\n",
    "    mm.utilitarian,\n",
    "    mm.utilitarian_anthro,\n",
    "    mm.action,\n",
    "    mm.pedestrians,\n",
    "    mm.females,\n",
    "    mm.fitness,\n",
    "    mm.status,\n",
    "    mm.legal,\n",
    "    mm.illegal,\n",
    "    mm.youth,\n",
    "#     mm.elderly,\n",
    "    mm.criminals,\n",
    "    mm.homeless,\n",
    "    mm.pets,\n",
    "    mm.spare_strollers,\n",
    "    mm.spare_pregnant\n",
    "]\n",
    "\n",
    "labeler = Labeler(lfs)\n",
    "L_train, L_dev, L_val = labeler.label([X_train, X_dev, X_val])\n",
    "LFAnalysis(L=L_dev, lfs=lfs).lf_summary(Y=y_dev.values).sort_values(\"Correct\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Experiment: LF Density*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the validation set (since tuning is done)\n",
    "analysis = LFAnalysis(L=L_val, lfs=lfs).lf_summary(Y=y_val.values)\n",
    "analysis.to_csv(\"../figures/data/lfanalysis.csv\")\n",
    "# labeling density\n",
    "pd.DataFrame(L_dev, columns=[lf.name for lf in lfs]).to_csv(\"../figures/data/mm-density.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation\n",
    "\n",
    "Recall that there are no true labels for this problem - really, we're just measuring similarity of the heuristic labels to real voter's responses. \n",
    "\n",
    "**Baseline**: majority label voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import MajorityLabelVoter\n",
    "\n",
    "model_majority = MajorityLabelVoter()\n",
    "preds_train = model_majority.predict(L=L_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Label Model**: Snorkel aggregator. Chooses weights to combine the labeling functions based on learned conditional probabilities.\n",
    "\n",
    "*Experiment: LF weights*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cardinality is num classes\n",
    "importlib.reload(hmm.labeling.models)\n",
    "\n",
    "model_label = labeler.fit(L_train, Y_dev=y_dev.values, fit_params={'n_epochs': 200, 'log_freq': 50})\n",
    "analysis = LFAnalysis(L=L_val, lfs=lfs).lf_summary(Y=y_val.values)\n",
    "analysis['weight'] = pd.Series(model_label.get_weights(), index=[lf.name for lf in lfs])\n",
    "analysis.to_csv('../figures/data/mm-weights.csv')\n",
    "analysis.sort_values('Emp. Acc.')\n",
    "# X[['Passenger_int', 'Passenger_noint', 'Law Abiding_int', 'Law Abiding_noint']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much does the label model improve on the majority voter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmm.labeling.models import Labeler\n",
    "importlib.reload(hmm.labeling.models)\n",
    "from hmm.labeling.models import Labeler\n",
    "\n",
    "for model in [model_majority, model_label]:\n",
    "    Labeler.score(model, L_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ranking by Effect Size*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import MajorityLabelVoter\n",
    "import scipy as sp\n",
    "\n",
    "class WeightedMajorityLabelVoter(MajorityLabelVoter):\n",
    "    def predict_proba(self, L: np.ndarray) -> np.ndarray:\n",
    "        n, m = L.shape\n",
    "        Y_p = np.zeros((n, self.cardinality))\n",
    "        for i in range(n):\n",
    "            counts = np.zeros(self.cardinality)\n",
    "            for j in range(m):\n",
    "                if L[i, j] != -1:\n",
    "                    # add a weighted count instead of a whole count\n",
    "                    counts[L[i, j]] += self.mu[j]\n",
    "            Y_p[i, :] = np.where(counts == max(counts), 1, 0)\n",
    "        Y_p /= Y_p.sum(axis=1).reshape(-1, 1)\n",
    "        return Y_p\n",
    "    \n",
    "    def interp_mu(self, borda, ordered_keys=None):\n",
    "        if ordered_keys is not None:\n",
    "            mu = borda.mean()[ordered_keys].values\n",
    "            self.set_mu(np.interp(mu, (borda.mean().min(), borda.mean().max()), (0, 1)))\n",
    "        else:\n",
    "            self.set_mu(np.interp(borda, (borda.min(), borda.max()), (0, 1)))\n",
    "            print(self.mu)\n",
    "    \n",
    "    def set_mu(self, mu):\n",
    "        self.mu = mu\n",
    "    \n",
    "    @staticmethod\n",
    "    def borda(x, key):\n",
    "        count = 0\n",
    "        key_val = x[x['key'] == key]['effect'].values[0]\n",
    "        for val in x[x['key'] != key]['effect'].values:\n",
    "            if key_val > val:\n",
    "                count += 1\n",
    "        return count\n",
    "        \n",
    "# TODO - try weighting this model by the learned LF bordas\n",
    "effect_sizes = pd.DataFrame([\n",
    "    ['action', 0.07],\n",
    "    ['pedestrians', .12],\n",
    "    ['females', .14],\n",
    "    ['fitness', .18],\n",
    "    ['status', .33],\n",
    "    ['legal', .35],\n",
    "    ['illegal', .35],\n",
    "    ['youth', 0.5],\n",
    "    ['utilitarian', 0.51],\n",
    "    ['utilitarian_anthro', 0.55],\n",
    "    ['spare_strollers', .18],\n",
    "    ['spare_pregnant', .15],\n",
    "    ['criminals', .12],\n",
    "    ['homeless', 0.02],\n",
    "    ['pets', 0.59],\n",
    "    ['doctors', 0.07]\n",
    "], columns=['key', 'effect'])\n",
    "effect_sizes['borda'] = effect_sizes.apply(lambda x: WeightedMajorityLabelVoter.borda(effect_sizes, x['key']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_majority_weighted = WeightedMajorityLabelVoter()\n",
    "borda = np.array([effect_sizes[effect_sizes['key'] == lf.name]['borda'].values[0] for lf in lfs])\n",
    "model_majority_weighted.interp_mu(borda)\n",
    "preds_train = model_majority_weighted.predict(L=L_train)\n",
    "\n",
    "analysis = LFAnalysis(L=L_val, lfs=lfs).lf_summary(Y=y_val.values)\n",
    "analysis['weight'] = pd.Series(model_majority_weighted.mu, index=[lf.name for lf in lfs])\n",
    "analysis.to_csv('../figures/data/mm-weights_icml.csv')\n",
    "analysis.sort_values('Emp. Acc.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much does the weighted majority voter improve on the majority voter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmm.labeling.models import Labeler\n",
    "importlib.reload(hmm.labeling.models)\n",
    "from hmm.labeling.models import Labeler\n",
    "\n",
    "for model in [model_majority_weighted, model_majority]:\n",
    "    Labeler.score(model, L_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Experiment: Accuracy by Scenario*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the label model accuracy per scenario type?\n",
    "# create a dataframe with scenariotype, gold label, probabilistic label, votes for each LF\n",
    "## TODO REMOVE DEPRECATED SECOND ARG\n",
    "preds_scenario = pd.DataFrame(L_val, columns=[lf.name for lf in lfs])\n",
    "preds_scenario['scenario'] = df_joined.loc[X_val.index]['ScenarioType'].values\n",
    "preds_scenario['actual'] = y_val.values\n",
    "probs = labeler.model.predict_proba(L=L_val)\n",
    "preds_scenario['pred'] = Labeler.probs_to_preds(probs)\n",
    "preds_scenario.to_csv(\"../figures/data/mm-preds_scenario.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eye Test - Debugging Label Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use the label model to create probabilistic labels for the dev set. Rounding off, create binary predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.analysis import get_label_buckets\n",
    "\n",
    "threshold = 0.5\n",
    "probs_dev = model_label.predict_proba(L=L_dev)\n",
    "preds_dev = probs_dev >= threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create label buckets for eyeball debugging (groups TP, FP, TN, FN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "print(labeler.get_confusion_matrix(L_dev, y_dev))\n",
    "buckets = labeler.get_label_buckets(L_dev, y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the accuracy for each scenario type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# false negatives\n",
    "df_fn_dev = X_dev.iloc[buckets[(1, 0)]]\n",
    "# false positives\n",
    "df_fp_dev = X_dev.iloc[buckets[(0, 1)]]\n",
    "df_n_dev = X_dev.iloc[np.concatenate([buckets[(1, 0)], buckets[(0, 1)]])]\n",
    "# acc = 1 - Neg / Total for each scenario type\n",
    "acc = 1 - df_joined.loc[df_n_dev.index, 'ScenarioType'].value_counts() / df_joined.loc[X_dev.index, 'ScenarioType'].value_counts()\n",
    "acc.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### False Negatives\n",
    "Here, the user chose to intervene, while the label model did not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the corresponding posteriori probability for each false negative\n",
    "df_fn_dev.loc[:,\"probability\"] = probs_dev[buckets[(1, 0)], 1]\n",
    "# check out a few\n",
    "pictofy(df_fn_dev.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which scenario types does the model tend to get wrong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined.loc[df_fn_dev.index, 'ScenarioType'].value_counts().plot.pie()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### False Positives\n",
    "Here, the user chose not to intervene, but the label model did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the corresponding posteriori probability for each false positive\n",
    "df_fp_dev.loc[:,\"probability\"] = probs_dev[buckets[(0, 1)], 1]\n",
    "# check out a few\n",
    "pictofy(df_fp_dev.sample(random_state=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined.loc[df_fp_dev.index, 'ScenarioType'].value_counts().plot.pie()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "https://www.snorkel.org/use-cases/01-spam-tutorial#5-training-a-classifier\n",
    "\n",
    "Let's design a simple machine learning classifier for this problem, then test it on both the gold standard labels and the heuristic labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hmm.classification\n",
    "importlib.reload(hmm.classification)\n",
    "from hmm.classification import Classifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = Classifier(features, num_features, cat_features)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=11)\n",
    "\n",
    "def kf_cross_val(kf, X_n, y_n):\n",
    "    gold_acc = []\n",
    "    lm_acc = []\n",
    "    mv_acc = []\n",
    "    mv_weighted_acc = []\n",
    "    \n",
    "    for i_train, i_test in kf.split(X_n):\n",
    "        # train/test split by fold\n",
    "        X_train_n, X_test_n = X_n.iloc[i_train], X_n.iloc[i_test]\n",
    "        y_train_n, y_test_n = y_n.iloc[i_train], y_n.iloc[i_test]\n",
    "        \n",
    "        # gold accuracy\n",
    "        clf.fit(X_train_n, y_train_n)\n",
    "        gold_acc.append(clf.score(X_test_n, y_test_n, verbose=False))\n",
    "        \n",
    "        # lm accuracy\n",
    "        ## train label model\n",
    "        labeler = Labeler(lfs)\n",
    "        L_train_n = labeler.label(X_train_n, verbose=False)\n",
    "        labeler.fit(L_train_n, Y_dev=y_train_n)\n",
    "        ## label points in X_train\n",
    "        X_train_filtered_n, probs_train_filtered_n = labeler.filter_probs(X_train_n, L_train_n)\n",
    "        preds_train_filtered_n = Labeler.probs_to_preds(probs_train_filtered_n)\n",
    "        ## fit and score\n",
    "        clf.fit(X_train_filtered_n, preds_train_filtered_n)\n",
    "        lm_acc.append(clf.score(X_test_n, y_test_n, verbose=False))\n",
    "        \n",
    "        L_test_n = labeler.label(X_test_n, verbose=False)\n",
    "        mv_acc.append(accuracy_score(y_true=y_test_n, y_pred=model_majority.predict(L=L_test_n)))\n",
    "        mv_weighted_acc.append(accuracy_score(y_true=y_test_n, y_pred=model_majority_weighted.predict(L=L_test_n)))\n",
    "        \n",
    "    return gold_acc, lm_acc\n",
    "\n",
    "sample = X.sample(10000).index\n",
    "gold_acc, lm_acc = kf_cross_val(kf, X.loc[sample], y.loc[sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training on Gold Labels\n",
    "\n",
    "Using just the labels (no label model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(gold_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training on Heuristic Labels\n",
    "\n",
    "Using the label model, filter out unlabeled points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(lm_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LF perturbations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline model\n",
    "labeler = Labeler(lfs)\n",
    "L_train, L_val = labeler.label([X_train, X_val], verbose=False)\n",
    "model_label = labeler.fit(L_train, Y_dev=y_train)\n",
    "acc_full = Labeler.score(model_label, L_val, y_val)\n",
    "# perturbed models\n",
    "lf_diffs = []\n",
    "for lf in lfs:\n",
    "    lfs_perturb = [l for l in lfs if l != lf]\n",
    "    l = Labeler(lfs_perturb)\n",
    "    L_train, L_val = l.label([X_train, X_val], verbose=False)\n",
    "    lm = labeler.fit(L_train)\n",
    "    acc_perturb = Labeler.score(lm, L_val, y_val, verbose=False)\n",
    "    lf_diffs.append((lf.name, acc_full - acc_perturb))\n",
    "    print(\"{}: {}\".format(lf.name, acc_full - acc_perturb))\n",
    "pd.DataFrame(lf_diffs, columns=['heuristic', 'value_added']).to_csv(\"../figures/data/mm-perturb.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which models perform best?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "for name, model in {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100),\n",
    "    \"Log Reg\": LogisticRegression(C=.001),\n",
    "    \"KNN\": KNeighborsClassifier(3),\n",
    "    \"SVC Linear\": SVC(kernel=\"linear\", C=0.025),\n",
    "    \"SVC Nonlinear\": SVC(gamma=2, C=1),\n",
    "    \"GP\": GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(max_depth=5),\n",
    "    \"RF\": RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    \"MLP\": MLPClassifier(alpha=1, max_iter=1000),\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"QDA\": QuadraticDiscriminantAnalysis()\n",
    "}.items():\n",
    "    print(\"## {} ##\".format(name))\n",
    "    clf = Classifier(features, num_features, cat_features, clf=model)\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    acc = clf.score(X_test, y_test, verbose=False)\n",
    "    print(\"Accuracy with gold labels: {}\".format(acc))\n",
    "    \n",
    "    clf.fit(X_train_filtered, preds_train_filtered)\n",
    "    acc = clf.score(X_test, y_test, verbose=False)\n",
    "    print(\"Accuracy with heuristic labels: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does performance change as the number of voters is increased?** \n",
    "\n",
    "In this case, evaluate performance voter-wise by splitting the data after stratifying by voter. Matches Noothigattu Fig. 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = []\n",
    "users = df_proc.groupby(['UserID'])\n",
    "a = np.arange(users.ngroups)\n",
    "\n",
    "trials = 200\n",
    "epochs = 50\n",
    "np.random.seed(21)\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    print(\"# Epoch {} #\".format(i))\n",
    "\n",
    "    acc_gold = []\n",
    "    acc_lm = []\n",
    "    \n",
    "    # shuffle the voter IDs\n",
    "    np.random.shuffle(a)\n",
    "    \n",
    "    num_range = list(range(1,5))+list(range(5,trials,5))\n",
    "    for n in num_range:\n",
    "#         print(\"## Testing N={}##\".format(n)) \n",
    "        n_respondents = df_proc[users.ngroup().isin(a[:n])]\n",
    "        train_index = n_respondents.groupby('UserID').head(8).index\n",
    "\n",
    "        X, y = make_X_y(n_respondents)\n",
    "        X_train, y_train = X.loc[train_index], y.loc[train_index]\n",
    "        X_test, y_test = X.loc[~X.index.isin(train_index)], y.loc[~y.index.isin(train_index)]\n",
    "        # label the data\n",
    "        L_train, L_test = labeler.label([X_train, X_test], verbose=False)\n",
    "\n",
    "        # baseline gold model accuracy\n",
    "        clf.fit(X_train, y_train)\n",
    "        acc_gold.append(clf.score(X_test, y_test, verbose=False))\n",
    "\n",
    "        # label model accuracy\n",
    "#         lm = labeler.fit(L_train)\n",
    "#         Labeler.score(lm, L_test, y_test, verbose=False)\n",
    "#         X_train_filtered, probs_train_filtered = labeler.filter_probs(X_train, L_train)\n",
    "#         preds_train_filtered = Labeler.probs_to_preds(probs_train_filtered)\n",
    "#         clf.fit(X_train_filtered, preds_train_filtered)\n",
    "#         acc_lm.append(clf.score(X_test, y_test, verbose=False))\n",
    "        \n",
    "        # unweighted majority vote accuracy\n",
    "        preds_train = model_majority.predict(L=L_train)\n",
    "        clf.fit(X_train, preds_train)\n",
    "        acc_lm.append(clf.score(X_test, y_test, verbose=False))\n",
    "\n",
    "    data.append([ \n",
    "        acc_gold,\n",
    "        acc_lm\n",
    "    ])\n",
    "summary = np.array(data)\n",
    "stats = np.concatenate((np.array([num_range]), np.mean(summary, axis=0), np.std(summary, axis=0)), axis=0).transpose()\n",
    "accs = pd.DataFrame(stats, columns=[\"n_voters\", \"acc_gold\", \"acc_heuristic\", \"std_gold\", \"std_heuristic\"])\n",
    "accs['n_voters'] = accs['n_voters'].astype(int)\n",
    "accs.to_csv(\"../figures/data/mm-accs_voters_icml.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs[['acc_gold', 'acc_heuristic']].plot(kind='line')\n",
    "accs.to_csv(\"../figures/data/mm-accs_voters_icml.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performance by training set size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "clf = Classifier(features, num_features, cat_features)\n",
    "results = []\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=11)\n",
    "\n",
    "for n in list(range(50, 1000, 10))+list(range(1100,10000,100)):\n",
    "    sample = X.sample(n).index\n",
    "    X_n = X.loc[sample]\n",
    "    y_n = y.loc[sample]\n",
    "    \n",
    "    gold_acc, lm_acc = kf_cross_val(kf, X_n, y_n)\n",
    "\n",
    "    # store results\n",
    "    res = (n, np.mean(gold_acc), np.std(gold_acc), np.mean(lm_acc), np.std(lm_acc))\n",
    "    print(res)\n",
    "    results.append(res)\n",
    "pd.DataFrame(results, columns=[\"n_rows\", \"acc_gold\", \"std_gold\", \"acc_heuristic\", \"std_heuristic\"]).to_csv(\"../figures/data/mm-accs_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
